# -*- coding: utf-8 -*-
"""Phase_B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xqTA417LtVyykEBoeTnNHTMZHEZQteon
"""

import os
!pip install torch==2.3.0+cpu --index-url https://download.pytorch.org/whl/cpu > /dev/null
!pip install  dgl==1.1.0 -f https://data.dgl.ai/wheels/repo.html > /dev/null
import torch
os.environ['TORCH'] = torch.__version__
os.environ['DGLBACKEND'] = "pytorch"

!pip install datasets
!pip install node2vec
!pip install -U scikit-learn
!pip install scikit-learn-extra

from google.colab import drive
import datetime
import os
drive.mount('/content/drive')

# Directory
directory = str(datetime.datetime.now()) + " BWGNN(hetero)" + " ISO with treshold = 5"

# Parent Directory path
parent_dir = '/content/drive/MyDrive/ColabNotebooks/Anom_Haar_pr/'

# Path
path = os.path.join(parent_dir, directory) + '/'
os.mkdir(path)
print(os.getcwd())

import dgl
import torch
import torch.nn.functional as F
import argparse
import time
from datasets import Dataset
from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, precision_score, confusion_matrix
from sklearn.model_selection import train_test_split
import os
import sys
import inspect
import networkx as nx
from node2vec import Node2Vec
from sklearn.cluster import KMeans
import pandas as pd
from sklearn_extra.cluster import KMedoids
from sklearn.ensemble import IsolationForest
from scipy.spatial import distance
from scipy.spatial.distance import cdist
from sklearn.neighbors import LocalOutlierFactor

import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl.function as fn
import math
import sympy
import scipy
import numpy as np
from torch import nn
from torch.nn import init
from dgl.nn.pytorch import GraphConv, EdgeWeightNorm, ChebConv, GATConv, HeteroGraphConv
import copy
import keras

class PolyConv(nn.Module):
    def __init__(self,
                 in_feats,
                 out_feats,
                 theta,
                 activation=F.leaky_relu,
                 lin=False,
                 bias=False):
        super(PolyConv, self).__init__()
        self._theta = theta
        self._k = len(self._theta)
        self._in_feats = in_feats
        self._out_feats = out_feats
        self.activation = activation
        self.linear = nn.Linear(in_feats, out_feats, bias)
        self.lin = lin
        # self.reset_parameters()
        # self.linear2 = nn.Linear(out_feats, out_feats, bias)

    def reset_parameters(self):
        if self.linear.weight is not None:
            init.xavier_uniform_(self.linear.weight)
        if self.linear.bias is not None:
            init.zeros_(self.linear.bias)

    def forward(self, graph, feat):
        def unnLaplacian(feat, D_invsqrt, graph):
            """ Operation Feat * D^-1/2 A D^-1/2 """
            graph.ndata['h'] = feat * D_invsqrt
            graph.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'h'))
            return feat - graph.ndata.pop('h') * D_invsqrt

        with graph.local_scope():
            D_invsqrt = torch.pow(graph.in_degrees().float().clamp(
                min=1), -0.5).unsqueeze(-1).to(feat.device)
            h = self._theta[0]*feat
            for k in range(1, self._k):
                feat = unnLaplacian(feat, D_invsqrt, graph)
                h += self._theta[k]*feat
        if self.lin:
            h = self.linear(h)
            h = self.activation(h)
        return h


class PolyConvBatch(nn.Module):
    def __init__(self,
                 in_feats,
                 out_feats,
                 theta,
                 activation=F.leaky_relu,
                 lin=False,
                 bias=False):
        super(PolyConvBatch, self).__init__()
        self._theta = theta
        self._k = len(self._theta)
        self._in_feats = in_feats
        self._out_feats = out_feats
        self.activation = activation

    def reset_parameters(self):
        if self.linear.weight is not None:
            init.xavier_uniform_(self.linear.weight)
        if self.linear.bias is not None:
            init.zeros_(self.linear.bias)

    def forward(self, block, feat):
        def unnLaplacian(feat, D_invsqrt, block):
            """ Operation Feat * D^-1/2 A D^-1/2 """
            block.srcdata['h'] = feat * D_invsqrt
            block.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'h'))
            return feat - block.srcdata.pop('h') * D_invsqrt

        with block.local_scope():
            D_invsqrt = torch.pow(block.out_degrees().float().clamp(
                min=1), -0.5).unsqueeze(-1).to(feat.device)
            h = self._theta[0]*feat
            for k in range(1, self._k):
                feat = unnLaplacian(feat, D_invsqrt, block)
                h += self._theta[k]*feat
        return h


def calculate_theta2(d):
    thetas = []
    x = sympy.symbols('x')
    for i in range(d+1):
        f = sympy.poly((x/2) ** i * (1 - x/2) ** (d-i) / (scipy.special.beta(i+1, d+1-i)))
        coeff = f.all_coeffs()
        inv_coeff = []
        for i in range(d+1):
            inv_coeff.append(float(coeff[d-i]))
        thetas.append(inv_coeff)
    return thetas


class BWGNN(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes, graph, d=2, batch=False):
        super(BWGNN, self).__init__()
        self.g = graph
        self.thetas = calculate_theta2(d=d)
        self.conv = []
        for i in range(len(self.thetas)):
            if not batch:
                self.conv.append(PolyConv(h_feats, h_feats, self.thetas[i], lin=False))
            else:
                self.conv.append(PolyConvBatch(h_feats, h_feats, self.thetas[i], lin=False))
        self.linear = nn.Linear(in_feats, h_feats)
        self.linear2 = nn.Linear(h_feats, h_feats)
        self.linear3 = nn.Linear(h_feats*len(self.conv), h_feats)
        self.linear4 = nn.Linear(h_feats, num_classes)
        self.act = nn.ReLU()
        self.d = d

    def forward(self, in_feat):
        h = self.linear(in_feat)
        h = self.act(h)
        h = self.linear2(h)
        h = self.act(h)
        h_final = torch.zeros([len(in_feat), 0])
        for conv in self.conv:
            h0 = conv(self.g, h)
            h_final = torch.cat([h_final, h0], -1)
            # print(h_final.shape)
        h = self.linear3(h_final)
        h = self.act(h)
        h = self.linear4(h)
        return h

    def testlarge(self, g, in_feat):
        h = self.linear(in_feat)
        h = self.act(h)
        h = self.linear2(h)
        h = self.act(h)
        h_final = torch.zeros([len(in_feat), 0])
        for conv in self.conv:
            h0 = conv(g, h)
            h_final = torch.cat([h_final, h0], -1)
            # print(h_final.shape)
        h = self.linear3(h_final)
        h = self.act(h)
        h = self.linear4(h)
        return h

    def batch(self, blocks, in_feat):
        h = self.linear(in_feat)
        h = self.act(h)
        h = self.linear2(h)
        h = self.act(h)

        h_final = torch.zeros([len(in_feat),0])
        for conv in self.conv:
            h0 = conv(blocks[0], h)
            h_final = torch.cat([h_final, h0], -1)
            # print(h_final.shape)
        h = self.linear3(h_final)
        h = self.act(h)
        h = self.linear4(h)
        return h


# heterogeneous graph
class BWGNN_Hetero(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes, graph, d=2):
        super(BWGNN_Hetero, self).__init__()
        self.g = graph
        self.thetas = calculate_theta2(d=d)
        self.h_feats = h_feats
        self.conv = [PolyConv(h_feats, h_feats, theta, lin=False) for theta in self.thetas]
        self.linear = nn.Linear(in_feats, h_feats)
        self.linear2 = nn.Linear(h_feats, h_feats)
        self.linear3 = nn.Linear(h_feats*len(self.conv), h_feats)
        self.linear4 = nn.Linear(h_feats, num_classes)
        self.act = nn.LeakyReLU()
        # print(self.thetas)

    def forward(self, in_feat):
        h = self.linear(in_feat)
        h = self.act(h)
        h = self.linear2(h)
        h = self.act(h)
        h_all = []

        for relation in self.g.canonical_etypes:
            # print(relation)
            h_final = torch.zeros([len(in_feat), 0])
            for conv in self.conv:
                h0 = conv(self.g[relation], h)
                h_final = torch.cat([h_final, h0], -1)
                # print(h_final.shape)
            h = self.linear3(h_final)
            h_all.append(h)

        h_all = torch.stack(h_all).sum(0)
        h_all = self.act(h_all)
        h_all = self.linear4(h_all)
        return h_all

def assign_labels(cora_graph,kernel_nodes):
  labels = np.zeros(cora_graph.num_nodes())
  for nodeID in range(cora_graph.num_nodes()):
    if(nodeID in kernel_nodes):
      labels[nodeID] = 1
    else:
      labels[nodeID] = 0
  return labels

def train(model, g, vectors, kernel_nodes, anomaly_nodes, train_results, args):
    features = torch.from_numpy(vectors)

    # Create labels: 1 for anomalies, 0 for non-anomalies
    labels = np.zeros(len(features), dtype=int)
    labels[anomaly_nodes] = 1
    print("regular      "+ str(labels))
    labels = torch.from_numpy(labels).type(torch.LongTensor)
    print(str(labels))
    index = list(range(len(labels)))
    print("index is     "+ str(index))
    idx_train, idx_rest, y_train, y_rest = train_test_split(index, labels[index], stratify=labels[index],
                                                            train_size=args.train_ratio,
                                                            random_state=2, shuffle=True)
    idx_valid, idx_test, y_valid, y_test = train_test_split(idx_rest, y_rest, stratify=y_rest,
                                                            test_size=0.67,
                                                            random_state=2, shuffle=True)
    train_mask = torch.zeros([len(labels)]).bool()
    val_mask = torch.zeros([len(labels)]).bool()
    test_mask = torch.zeros([len(labels)]).bool()

    train_mask[idx_train] = 1
    val_mask[idx_valid] = 1
    test_mask[idx_test] = 1
    print('train/dev/test samples: ', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    best_f1, final_tf1, final_trec, final_tpre, final_tmf1, final_tauc = 0., 0., 0., 0., 0., 0.

    weight = (1-labels[train_mask]).sum().item() / labels[train_mask].sum().item()
    print('cross entropy weight: ', weight)
    time_start = time.time()
    for e in range(args.epoch):
        model.train()
        logits = model(features)
        # print((logits[train_mask]))
        # print((labels[train_mask]))
        # print(logits)
        loss = F.cross_entropy(logits[train_mask], labels[train_mask])
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        model.eval()
        probs = logits.softmax(1)
        train_results.append(probs)
        f1, thres = get_best_f1(labels[val_mask], probs[val_mask])
        preds = np.zeros_like(labels)
        preds[probs[:, 1] > thres] = 1
        print(preds)
        trec = recall_score(labels[test_mask], preds[test_mask],average='macro')
        tpre = precision_score(labels[test_mask], preds[test_mask],average='macro')
        tmf1 = f1_score(labels[test_mask], preds[test_mask], average='macro')
        tauc = roc_auc_score(labels[test_mask], probs[test_mask].detach().numpy()[:,1],multi_class='ovr')

        if best_f1 < f1:
            best_f1 = f1
            final_trec = trec
            final_tpre = tpre
            final_tmf1 = tmf1
            final_tauc = tauc
        #print('Epoch {}, loss: {:.4f}, val mf1: {:.4f}, (best {:.4f})'.format(e, loss, f1, best_f1))

    time_end = time.time()
    #print('time cost: ', time_end - time_start, 's')
    #print('Test: REC {:.2f} PRE {:.2f} MF1 {:.2f} AUC {:.2f}'.format(final_trec*100,
                                                                     #final_tpre*100, final_tmf1*100, final_tauc*100))
    return final_tmf1, final_tauc, preds


# threshold adjusting for best macro f1
def get_best_f1(labels, probs):
    best_f1, best_thre = 0, 0
    for thres in np.linspace(0.05, 0.95, 19):
        preds = np.zeros_like(labels)
        preds[probs[:,1] > thres] = 1
        mf1 = f1_score(labels, preds, average='macro')
        if mf1 > best_f1:
            best_f1 = mf1
            best_thre = thres
    return best_f1, best_thre

def build_nx_Graph(cora_graph):
  nodes = np.array([node.item() for node in cora_graph.nodes()])
  edges_tuple = cora_graph.edges() # there is an edge between the first tensor at position i and the second tensor at position i
  num_edges = cora_graph.number_of_edges()

  edges_list_of_tuples = [(edges_tuple[0][i].item(),edges_tuple[1][i].item()) for i in range(num_edges)]
  # create a suitable graph for node to vec.
  G = nx.Graph()

  for node in nodes:
    G.add_node(node,feat=cora_graph.ndata['feat'][node])

  G.add_edges_from(edges_list_of_tuples)
  return G

def generate_node2vec_embeddings(G):
  node2vec = Node2Vec(G, dimensions=64, walk_length=50, num_walks=200, workers=4)
  # Embed nodes
  model = node2vec.fit(window=10, min_count=1, batch_words=4)

  # Retrieve the embeddings for all nodes
  embeddings = {node: model.wv[node] for node in G.nodes()}
  # Example: Print the embedding for node 0
  print("Embedding for Node 0:", embeddings[0])
  # save embbedings in a .npy file.
  np.save(path + 'embeddings.npy', list(embeddings.values()))
  return embeddings

def calcMaxDist(i,cluster_nodes_dict):
    cluster_center = iso_forest_results.cluster_centers_[i]
    dmax = 0
    for node_ID in cluster_nodes_dict[i]:
      dmax = max(dmax,np.linalg.norm(cluster_center - vectors[node_ID])) # maximal distance of point within the cluster from the center of the cluster.
      # dmax = max(dmax,distance.cosine(cluster_center,vectors[node_ID])) # maximal distance of point within the cluster from the center of the cluster.
    return dmax

def filterMedoidKernel(i,threshold,dmax,cluster_nodes_dict):
  # threshold parameter should be specified as the threshold percentage of max distance which is the radius of the kernel.
  cluster_center = kmedoids_results.cluster_centers_[i]

  residual_nodes = []
  kernel_nodes = []
  for node_ID in cluster_nodes_dict[i]:
    # if the node_ID is too far from the kernel - remove it from the cluster.
    if(np.linalg.norm(cluster_center - vectors[node_ID]) > threshold * dmax):
       cluster_nodes_dict[i] = np.setdiff1d(cluster_nodes_dict[i],[node_ID])
       residual_nodes.append(node_ID)
    else:
      kernel_nodes.append(node_ID)
  return kernel_nodes,residual_nodes

def create_kernel_and_residual_sets(kmedoids_results,dmax_arr):
    # med_clusters = pd.Series(kmedoids_results.labels_)
    # cluster_nodes_dict = {i:np.array(med_clusters[med_clusters == i].index) for i in range(kmedoids.n_clusters)} # keys -> clusters, values -> nodes ID's.
    # dmax_arr = [calcMaxDist(i,cluster_nodes_dict) for i in cluster_nodes_dict]
    print('original number of nodes in clusters:',end='')
    for k in range(kmedoids.n_clusters):
      print(len(cluster_nodes_dict[k]),end=' ')

    residual_nodes = set()
    kernel_nodes = set()
    for i in cluster_nodes_dict:
      curr_kernel_nodes,curr_residual_nodes = filterMedoidKernel(i,threshold,dmax_arr[i],cluster_nodes_dict)
      residual_nodes.update(curr_residual_nodes)
      kernel_nodes.update(curr_kernel_nodes)

    print()
    print('filtered number of nodes in clusters:',end='')
    for k in range(7):
      print(len(cluster_nodes_dict[k]),end=' ')

    return kernel_nodes, residual_nodes

# # create embedding vectors
original_graph = dgl.data.CoraGraphDataset()[0]
# G = build_nx_Graph(original_graph)
# embeddings = generate_node2vec_embeddings(G)

def getRealNodeID(node,original_node_IDs):
  # node is the ID of the newly relabeled node
  # original_node_IDs is the array of the original node labeling.
  return original_node_IDs[node]

import matplotlib.pyplot as plt
import numpy as np
#creating an histogrem graph of the isoation forest scores, in a file for every iteration.
def createHistogramBasedOnIsolatedForestScores(iso_forest_scores, iter_num, need_to_mkdir):
  if need_to_mkdir:
      os.mkdir(path + "Histogram Results")
  path_to_histogram = os.path.join(path, "Histogram Results") + '/'

  # Define custom bin edges for precise analysis
  bin_edges = [0, 0.03, 0.06, 0.09, 0.12, 0.15, 0.18, 0.2]

  # Create a histogram
  n, bins, patches = plt.hist(iso_forest_scores, bins=bin_edges)

  # Assign a color to each patch
  color_map = ['red', 'green', 'blue', 'purple', 'brown', 'magenta', 'cyan']

  for patch, color in zip(patches, color_map):
      patch.set_facecolor(color)

  # Create a histogram
  #plt.hist(iso_forest_scores, bins=bin_edges)
  plt.xlabel('Anomaly Score')
  plt.ylabel('Number of Nodes')
  plt.title(f'Histogram of Isolation Forest Scores (Iteration {iter_num})')

  # Save the histogram as an image
  plt.savefig(path_to_histogram + f'histogram_iteration_{iter_num+1}.png')
  plt.close()

original_graph

# Create Table with Node ID's of anomalous nodes and their respective Edge Count.
def createEdgesTable(unique_final_res_nodes,iter_num):
  edges = [len(G.edges(node)) for node in unique_final_res_nodes]
  #cluster_num = iso_forest_labels[unique_final_res_nodes]

  df_dict = dict()
  nodes = [getRealNodeID(node,original_node_IDs) for node in unique_final_res_nodes]
  df_dict['Node ID'] = nodes
  df_dict['Degree'] = edges
  #df_dict['Cluster Number'] = cluster_num
  df_dict[f'Iteration {iter_num}'] = 1
  df_dict['Is Anomalous'] = 1

  anomalous_nodes_table = pd.DataFrame(df_dict)

  # arange the colmuns of the dataframe
  cols = anomalous_nodes_table.columns.tolist()
  i = cols.index(f'Iteration {iter_num}')
  j = cols.index('Is Anomalous')
  temp = cols[i]
  cols[i] = cols[j]
  cols[j] = temp

  anomalous_nodes_table.columns = cols
  anomalous_nodes_table.set_index('Node ID',inplace = True)
  return anomalous_nodes_table

def saveFinalResults():
  anomalies_table = pd.concat(tables_results)
  anomalies_table

  # add kernel nodes to DataFrame
  # 1. build kernel dataframe
  kernel_table_dict = dict()
  kernel_nodes = [node for node in range(original_graph.number_of_nodes()) if node not in all_anomalious_nodes]
  kernel_table_dict['Node ID'] = kernel_nodes
  kernel_table_dict['Degree'] = [len((original_graph.out_edges(node,'uv'))[0]) for node in kernel_nodes]
  #kernel_table_dict['Cluster Number'] = initial_clustering_results[kernel_nodes]
  kernel_table_dict['Is Anomalous'] = 0

  kernel_table = pd.DataFrame(kernel_table_dict)
  kernel_table.set_index('Node ID',inplace = True)

  # 2. concat kernel nodes table with anomalous nodes table
  final_results = pd.concat([kernel_table,anomalies_table])
  final_results.fillna(0,inplace = True)
  final_results.sort_index(inplace = True)

  final_results.to_csv(path + 'Final Results.csv')
  return final_results

# get the papers data for subjects labeling assignment.
zip_file = keras.utils.get_file(
    fname="cora.tgz",
    origin="https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz",
    extract=True,
)
data_dir = os.path.join(os.path.dirname(zip_file), "cora")
column_names = ["paper_id"] + [f"term_{idx}" for idx in range(1433)] + ["subject"]
papers = pd.read_csv(
    os.path.join(data_dir, "cora.content"), sep="\t", header=None, names=column_names,
)
print("Papers shape:", papers.shape)

# create node to subject mapping for correct cluster number assignment.
node_to_subject_mapping = papers['subject']

# enumerate the subjects.
class_values = sorted(papers["subject"].unique())
class_idx = {name: id for id, name in enumerate(class_values)}

node_to_subject_mapping = node_to_subject_mapping.apply(lambda subjectName: class_idx[subjectName])
node_to_subject_mapping

import argparse
import copy
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from scipy.spatial.distance import cdist

anomalies_count = [0, 0, 0, 0]

if __name__ == '__main__':
  parser = argparse.ArgumentParser(description='BWGNN')
  parser.add_argument("-f", required=False)
  parser.add_argument("--dataset", type=str, default="cora", help="Dataset for this model (yelp/amazon/tfinance/tsocial)")
  parser.add_argument("--train_ratio", type=float, default=0.4, help="Training ratio")
  parser.add_argument("--hid_dim", type=int, default=64, help="Hidden layer dimension")
  parser.add_argument("--order", type=int, default=2, help="Order C in Beta Wavelet")
  parser.add_argument("--homo", type=int, default=0, help="1 for BWGNN(Homo) and 0 for BWGNN(Hetero)")
  parser.add_argument("--epoch", type=int, default=100, help="The max number of epochs")
  parser.add_argument("--run", type=int, default=1, help="Running times")

  args = parser.parse_args()
  print(args)
  dataset_name = args.dataset
  homo = args.homo
  order = args.order
  h_feats = args.hid_dim
  dgl_data = dgl.data.CoraGraphDataset()
  original_graph = dgl_data[0]

  in_feats = 64  # dimension of node2vec vectors
  num_classes = dgl_data.num_classes

  train_results = []
  tables_results = []

  initial_clustering_results = None

  all_anomalious_nodes = set()

  if args.run == 1:
    if homo:
      model = BWGNN(in_feats, h_feats, num_classes, original_graph, d=order)
    else:
      model = BWGNN_Hetero(in_feats, h_feats, num_classes, original_graph, d=order)

    # Algorithm steps
    curr_graph = copy.copy(original_graph)

    dmax_arr = None
    num_iterations = 4
    original_node_IDs = [node.item() for node in curr_graph.nodes()]

    cluster_history = pd.DataFrame()
    with open(path + 'Cluster Results with Ground Truth Class Labeling.txt', 'w') as f:
      f.write(str(class_values) + '\n')

    for i in range(num_iterations):
      # 1. Build nx graph G from previous graph copy.
      dgl_graph = copy.copy(curr_graph)
      G = build_nx_Graph(dgl_graph)

      # 2. Generate Vector Embeddings
      embeddings = generate_node2vec_embeddings(G)
      vectors = np.load(path + 'embeddings.npy', allow_pickle=True)  # vector at index i represents a node with ID = i
      print("vectors     "+ str(vectors))

      # 3. Run Isolation Forest on vectors matrix

      iso_forest = IsolationForest(n_estimators=100)
      iso_forest_results = iso_forest.fit(vectors)
      iso_forest_scores = iso_forest.decision_function(vectors)
      print("scors       "+ str(iso_forest_scores))
      print("results      "+ str(iso_forest_results))

      createHistogramBasedOnIsolatedForestScores(iso_forest_scores, i, (i == 0))

      # Calculate the threshold for the bottom 5% of scores
      num_samples = iso_forest_scores.shape[0]
      threshold = np.percentile(iso_forest_scores, 5)#5%, 20%, 10
      print("threshold        " + str(threshold))
      print("num_samples       "+ str(num_samples))
      # Identify anomalous nodes
      anomalous_nodes = np.where(iso_forest_scores <= threshold)[0]
      normal_nodes = np.where(iso_forest_scores > threshold)[0]

      # 5. Create kernel and residual sets of nodes
      kernel_nodes, residual_nodes = normal_nodes, anomalous_nodes
      print("kernel    " + str(kernel_nodes))
      print("residual         " + str(residual_nodes))

          # 6. Train GNN
      if homo:
        model = BWGNN(in_feats, h_feats, num_classes, dgl_graph, d=order)
      else:
        model = BWGNN_Hetero(in_feats, h_feats, num_classes, dgl_graph, d=order)

      _, _, preds = train(model, dgl_graph, vectors, kernel_nodes,residual_nodes, train_results, args)
      predictions = pd.Series(preds)

      # 7. Obtain final anomalous nodes and save them into a file
      initial_anomalous_nodes = set(residual_nodes) #IF anomalies
      final_anomalous_nodes = set(predictions[predictions == 1].index) #BWGNN anomalies
      print("final_anomalous_nodes    " + str(final_anomalous_nodes))

      unique_final_res_nodes = [node for node in final_anomalous_nodes if node not in initial_anomalous_nodes]
      print("unique_final_res_nodes    " + str(unique_final_res_nodes))

      real_anomal_ids = [getRealNodeID(anomalNode, original_node_IDs) for anomalNode in unique_final_res_nodes]
      all_anomalious_nodes |= set(real_anomal_ids)

      print("i is:    " + str(i))
      display(createEdgesTable(unique_final_res_nodes, i))
      tables_results.append(createEdgesTable(unique_final_res_nodes, i))

      # 8. Remove final anomalous nodes from graph copy.
      # after removal - automatic relabeling of the nodes is applied.
      final_anomalous_nodes_list = list(final_anomalous_nodes)
      anomalies_count[i]= len(final_anomalous_nodes_list)

      curr_graph.remove_nodes(final_anomalous_nodes_list)
      #curr_graph.remove_nodes(final_anomalous_nodes)
      real_nodes_to_remove = [getRealNodeID(node_to_remove,original_node_IDs) for node_to_remove in final_anomalous_nodes]
      for node_to_remove in real_nodes_to_remove:
        original_node_IDs.remove(node_to_remove)

      cluster_history.to_csv(path + 'Cluster with Most Common Ground Truth Labels.csv')
      saveFinalResults()

# acess final results file
print(os.getcwd())
df = pd.read_csv(path + 'Final Results.csv')
cols = df.columns.to_list()
for i in range(len(cols)):
  if('Iteration' in cols[i]):
    iterNum = int(cols[i].split()[1])
    cols[i] = f'Iteration {iterNum+1}'
df.columns = cols

# Save iteration results about anomalous in seperate tables.
dirname = path + "Iteration Results/"
if not os.path.exists(dirname):
  os.mkdir(dirname)
iter_cols = df.columns.to_list()[4:]
main_cols = df.columns.to_list()[:4]
for i in range(len(iter_cols)):
  print(iter_cols[i-1])
  display_cols = main_cols + [iter_cols[i-1]]
  df_with_iter = df[display_cols]
  df_res = df_with_iter[(df_with_iter['Is Anomalous'] == 1) & (df_with_iter[iter_cols[i-1]] == 1)]
  display(df_res)
  df_res.to_csv(dirname + f'{iter_cols[i-1]} results.csv')

data = {}
for iter_num in range(len(anomalies_count)):
  data[f'Iteration {iter_num + 1}'] = [anomalies_count[iter_num]]

print("anomalies_count is:      ")
print(anomalies_count)
print()

print("data for Iteration anomalies.csv is: ")
print()
print(data)
df_iterations = pd.DataFrame(data)
df_iterations.to_csv(path + 'Iteration anomalies.csv', index=False)

# size = len(class_values)
# matrix = np.zeros((size,size))
# for i in range(size):
#   # nodes with real label i (as saved in the dataset).
#   nodes_with_real_label_i = node_to_subject_mapping[node_to_subject_mapping == i].index.to_list()

#   for j in range(size):
#     # nodes which were added into cluster j by the algorithm
#     nodes_assigned_into_cluster_j = df[df['Cluster Number'] == j]

#     # find nodes which were added into cluster j and they are also labeled as cluster i initially.
#     mask = nodes_assigned_into_cluster_j['Node ID'].isin(nodes_with_real_label_i)
#     num_nodes_in_i_j = nodes_assigned_into_cluster_j[mask].shape[0]
#     matrix[i][j] = num_nodes_in_i_j

# labeling_results_df = pd.DataFrame(matrix)
# # rows = True Labels
# # cols = Our Labeling
# # example: matrix[0][1] => Number of nodes with initial labeling of 0 that were assigned to cluster 1.
# labeling_results_df.to_csv(path + 'labeling results.csv')

path

